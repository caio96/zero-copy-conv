#!/usr/bin/env python3

import argparse
import sys
from pathlib import Path
from tabulate import tabulate

import pandas as pd

def summarize_correctness(df: pd.DataFrame, output_dir, tolerance, only_stats=False):
    # Add tolerance
    df["tolerance"] = tolerance

    # Save all conv parameters that had incorrect results
    incorrect_df = df.loc[(df["max_diff"] > tolerance) & (df["error_occurred"] != True)]
    incorrect_df = incorrect_df.drop(columns=["error_message", "error_occurred"])
    incorrect_df = incorrect_df.sort_values(["conv_type", "max_diff"], ascending=False)
    if not only_stats:
        incorrect_df.to_csv(output_dir / "incorrect-convolutions.csv", index=False)

    # Separate df by 'conv_type'
    groups = df.groupby(by=["conv_type"])
    summary_diffs = []
    summary_methods = []
    summary_incorrect_results = []
    summary_error_results = []
    summary_correct_results = []
    for name, group in groups:
        name = name[0]
        summary_methods.append(name)

        summary_diffs.append(group["max_diff"].max())

        incorrect_convs = group.loc[(group["max_diff"] > tolerance) & (group["error_occurred"] != True)].shape[0]
        summary_incorrect_results.append(incorrect_convs)

        error_convs = group.loc[group["error_occurred"] == True].shape[0]
        summary_error_results.append(error_convs)

        total_convs = group.shape[0]
        summary_correct_results.append(total_convs - incorrect_convs - error_convs)

    # Construct summary
    summary = {
        "Method": summary_methods,
        "Maximum Difference": summary_diffs,
        "Correct Results": summary_correct_results,
        "Incorrect Results": summary_incorrect_results,
        "Error Results": summary_error_results,
        "Tolerance": [tolerance] * len(summary_methods)
    }
    summary = pd.DataFrame(summary).set_index("Method").sort_values(by="Method")
    print(tabulate(summary, headers="keys", tablefmt="psql"))

    if not only_stats:
        summary.to_csv(output_dir / "correctness-summary.csv", index=False)


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description="Summarize correctness results into CSVs."
    )

    parser.add_argument("CSV_Results", type=str, help="Path to the input CSV file (generated by benchmark_runner).")
    parser.add_argument(
        "Output_Dir", type=str, help="Path to directory to store outputs."
    )
    parser.add_argument(
        "--tolerance",
        type=float,
        help="Maximum difference tolerance between results. Default is 0.125.",
        default=0.125,
    )
    parser.add_argument(
        "--only-stats",
        action="store_true",
        help="Do not save csv files or graphs, only print stats",
    )

    args = parser.parse_args()

    csv_results = Path(args.CSV_Results)
    output_dir = Path(args.Output_Dir)
    tolerance = args.tolerance
    only_stats = args.only_stats

    # Check if csv file exists
    if (not csv_results.exists()) or (not csv_results.is_file()):
        print("CSV with results not found.", file=sys.stderr)
        sys.exit(-1)

    # Check if output dir exists
    if (not output_dir.exists()) or (not output_dir.is_dir()):
        print("Output directory not found.", file=sys.stderr)
        sys.exit(-1)

    df = pd.read_csv(csv_results, header=0, index_col=False)
    summarize_correctness(df, output_dir, tolerance, only_stats)
