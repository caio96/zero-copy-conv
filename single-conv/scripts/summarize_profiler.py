#!/usr/bin/env python3

import argparse
import itertools
import sys
from pathlib import Path
from collections import defaultdict

import matplotlib.pyplot as plt
from matplotlib import rc
import numpy as np
import pandas as pd
from filter_csv import exclude_from_df, include_only_in_df, split_parameters, get_categories
from tabulate import tabulate
import scipy.stats as st
from matplotlib.ticker import FuncFormatter


def perf_log_to_df(input_log : Path):

    data = defaultdict(list)
    counters = set()
    with open(input_log, 'r') as f:
        perf_stat_seen = 0
        for line in f:
            if line.strip() == '' or "started on" in line:
                continue

            if "/process_time/real_time" in line:
                id = line.split(',')[0].replace('"', '').split('/')[0]
                conv_type = id.split(' ')[0]
                conv_parameters = " ".join(id.split(' ')[1:])
                data['conv_parameters'].append(conv_parameters)
                data['conv_type'].append(conv_type)
                data['error_occurred'].append(line.split(',')[8])
            else:
                counter = line.split(',')[2]
                counters.add(counter)
                count = line.split(',')[0]
                data[counter].append(count)

    df = pd.DataFrame(data)

    # Remove rows where error_occurred is True (happens when Yaconv is not supported)
    df = df.loc[df["error_occurred"] != "true"]

    # Separate df by 'conv_type'
    groups = df.groupby(by=["conv_type"])
    df_dict = {}
    for name, group in groups:
        name = name[0]
        df_dict[name] = group.sort_values(by=["conv_parameters"]).reset_index(drop=True)

    # Join results by 'conv_parameters'
    method_names = list(df_dict.keys())

    if len(method_names) == 1:
        print("Only one method found. No comparison possible.", file=sys.stderr)
        sys.exit(-1)

    for counter in counters:
        joined_results = pd.merge(
            df_dict[method_names[0]][["conv_parameters", counter]].rename(columns={counter: method_names[0]}),
            df_dict[method_names[1]][["conv_parameters", counter]].rename(columns={counter: method_names[1]}),
            how="left",
            on="conv_parameters",
            suffixes=(None, None),
        )
        for method_name in method_names[2:]:
            joined_results = joined_results.merge(
                df_dict[method_name][["conv_parameters", counter]].rename(columns={counter: method_name}),
                how="left",
                on="conv_parameters",
                suffixes=(None, None),
            )

        joined_results = joined_results.rename(columns={"conv_parameters": counter}).set_index(counter)
        joined_results = joined_results.transpose().reset_index().rename(columns={"index": counter}).set_index(counter)
        print(tabulate(joined_results, headers="keys", tablefmt="psql"))


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description="Parse output log with profiler results and summarize them into a table."
    )

    parser.add_argument(
        "Log_Input", type=str, help="Path to the input log file (generated by benchmark_runner with --save-profile)."
    )
    parser.add_argument("Output_Dir", type=str, help="Path to directory to store outputs.")

    args = parser.parse_args()

    input_log = Path(args.Log_Input)
    output_dir = Path(args.Output_Dir)

    # Check if csv file exists
    if (not input_log.exists()) or (not input_log.is_file()):
        print("CSV with results not found.", file=sys.stderr)
        sys.exit(-1)

    # Check if output dir exists
    if (not output_dir.exists()) or (not output_dir.is_dir()):
        print("Output directory not found.", file=sys.stderr)
        sys.exit(-1)

    df = perf_log_to_df(input_log)

